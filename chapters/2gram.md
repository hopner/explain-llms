# The 2-gram model
**Learning objective:** How considering pairs of words improves text generation.

Content table:
| Step | Content | Animation |
|---|---:|---|
| 1 | Ignores context of words | Show histogram from 1-gram model |
| 2 | Considers pairs of words (bigrams) | Morph histogram into bigram table |
| 3 | Now there is a some sense of context | The network is created showing connections between word pairs |
| 4 | Learning probabilities of word pairs | Thicker connections for more probable pairs |
| 5 | Generates more coherent text | Given a prompt, the model generates text by following bigram probabilities |
| 6 | Still limited, but a step closer to natural language | River bank heist or other example |